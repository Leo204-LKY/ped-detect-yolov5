"""
Generated by Claude

This script performs real-time person detection using an ONNX model with OpenCV.
It initializes the model, captures video from the camera, detects persons in each frame, and displays the results with bounding boxes and confidence scores.
"""

import cv2
import numpy as np
import time
import onnxruntime as ort

class PersonDetector:
    def __init__(self, model_path='model.onnx', conf_threshold=0.5, nms_threshold=0.4):
        """
        Initialize person detector
        
        Args:
            model_path: ONNX model file path
            conf_threshold: Confidence threshold
            nms_threshold: NMS threshold
        """
        self.conf_threshold = conf_threshold
        self.nms_threshold = nms_threshold
        self.input_size = 640  # Model expects 640x640 input
        
        # Load ONNX model with ONNXRuntime
        # Try CUDA first, fallback to CPU
        try:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            self.session = ort.InferenceSession(model_path, providers=providers)
            if 'CUDAExecutionProvider' in self.session.get_providers():
                print("Using CUDA GPU acceleration")
            else:
                print("CUDA not available, using CPU")
        except Exception as e:
            print(f"CUDA provider failed, using CPU only: {e}")
            providers = ['CPUExecutionProvider']
            self.session = ort.InferenceSession(model_path, providers=providers)
        
        # Get input and output names
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        # Print model input shape for verification
        input_shape = self.session.get_inputs()[0].shape
        print(f"Active providers: {self.session.get_providers()}")
        print(f"Model expected input shape: {input_shape}")
        print(f"Model input size: {self.input_size}x{self.input_size}")
        print(f"Input name: {self.input_name}")
        print(f"Output names: {self.output_names}")
    
    def preprocess(self, image):
        """
        Image preprocessing
        
        Args:
            image: Input image
            
        Returns:
            input_tensor: Preprocessed tensor
            scale: Scale ratio
            pad_x: Padding offset in x direction
            pad_y: Padding offset in y direction
        """
        # Get original image dimensions
        height, width = image.shape[:2]
        
        # Calculate scale ratio to maintain aspect ratio
        scale = self.input_size / max(width, height)
        
        # Resize image
        new_width = int(width * scale)
        new_height = int(height * scale)
        resized_image = cv2.resize(image, (new_width, new_height))
        
        # Create padded image
        padded_image = np.ones((self.input_size, self.input_size, 3), dtype=np.uint8) * 114
        
        # Calculate padding offsets
        pad_x = (self.input_size - new_width) // 2
        pad_y = (self.input_size - new_height) // 2
        
        # Place resized image in center
        padded_image[pad_y:pad_y+new_height, pad_x:pad_x+new_width] = resized_image
        
        # Convert to tensor format
        input_tensor = padded_image.astype(np.float32) / 255.0
        input_tensor = np.transpose(input_tensor, (2, 0, 1))  # HWC to CHW
        input_tensor = np.expand_dims(input_tensor, axis=0)  # Add batch dimension
        
        return input_tensor, scale, pad_x, pad_y
    
    def postprocess(self, outputs, scale, original_shape, pad_x=None, pad_y=None):
        """
        Post-process detection results
        
        Args:
            outputs: Model outputs
            scale: Scale ratio
            original_shape: Original image dimensions
            pad_x: Padding offset in x direction
            pad_y: Padding offset in y direction
            
        Returns:
            boxes: Detection boxes
            confidences: Confidence scores
            class_ids: Class IDs
        """
        boxes = []
        confidences = []
        class_ids = []
        
        # Get the first output (assuming single output)
        output = outputs[0]
        
        # Handle different output shapes
        if len(output.shape) == 3:
            output = output[0]
        
        # Parse detections
        for detection in output:
            # YOLOv5 format: [x, y, w, h, confidence, class_scores...]
            confidence = detection[4]
            
            if confidence > self.conf_threshold:
                # Get class scores
                class_scores = detection[5:]
                class_id = np.argmax(class_scores)
                class_confidence = class_scores[class_id]
                
                # Final confidence is objectness * class confidence
                final_confidence = confidence * class_confidence
                
                if final_confidence > self.conf_threshold:
                    # Get bounding box coordinates (center format)
                    center_x = detection[0]
                    center_y = detection[1]
                    width = detection[2]
                    height = detection[3]
                    
                    # Remove padding offset before scaling back
                    if pad_x is not None and pad_y is not None:
                        center_x = center_x - pad_x
                        center_y = center_y - pad_y
                    
                    # Convert to top-left corner format and scale back
                    x = int((center_x - width / 2) / scale)
                    y = int((center_y - height / 2) / scale)
                    w = int(width / scale)
                    h = int(height / scale)
                    
                    boxes.append([x, y, w, h])
                    confidences.append(float(final_confidence))
                    class_ids.append(class_id)
        
        # Apply NMS
        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.conf_threshold, self.nms_threshold)
        
        if len(indices) > 0:
            indices = indices.flatten()
            return (
                [boxes[i] for i in indices],
                [confidences[i] for i in indices],
                [class_ids[i] for i in indices]
            )
        
        return [], [], []

    def detect(self, image):
        """
        Detect persons
        
        Args:
            image: Input image
            
        Returns:
            boxes: Detection boxes
            confidences: Confidence scores
            class_ids: Class IDs
        """
        # Preprocess
        input_tensor, scale, pad_x, pad_y = self.preprocess(image)
        
        # Inference with ONNXRuntime
        try:
            outputs = self.session.run(self.output_names, {self.input_name: input_tensor})
            # Debug: print output shape (only first time)
            if not hasattr(self, '_shape_printed'):
                print(f"Model output shape: {[output.shape for output in outputs]}")
                self._shape_printed = True
        except Exception as e:
            print(f"Inference error: {e}")
            return [], [], []
        
        # Post-process
        boxes, confidences, class_ids = self.postprocess(outputs, scale, image.shape[:2], pad_x, pad_y)
        
        return boxes, confidences, class_ids

    def draw_detections(self, image, boxes, confidences, class_ids):
        """
        Draw detection results
        
        Args:
            image: Input image
            boxes: Detection boxes
            confidences: Confidence scores
            class_ids: Class IDs
            
        Returns:
            annotated_image: Annotated image
        """
        annotated_image = image.copy()
        
        for i, (box, conf, class_id) in enumerate(zip(boxes, confidences, class_ids)):
            x, y, w, h = box
            
            # Draw bounding box
            cv2.rectangle(annotated_image, (x, y), (x + w, y + h), (0, 255, 0), 2)
            
            # Draw label
            label = f'Person: {conf:.2f}'
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(annotated_image, (x, y - label_size[1] - 10), 
                         (x + label_size[0], y), (0, 255, 0), -1)
            cv2.putText(annotated_image, label, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        return annotated_image

def main():
    """Main function"""
    # Initialize detector
    try:
        detector = PersonDetector('model.onnx')
        print("Model loaded successfully")
    except Exception as e:
        print(f"Model loading failed: {e}")
        return
    
    # Open camera
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Cannot open camera")
        return
    
    # Set camera resolution
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    print("Starting real-time detection, press 'q' to exit")
    
    # FPS counter
    fps_counter = 0
    start_time = time.time()
    
    while True:
        # Read frame
        ret, frame = cap.read()
        if not ret:
            print("Cannot read camera data")
            break
        
        # Detect persons
        boxes, confidences, class_ids = detector.detect(frame)
        
        # Draw detection results
        annotated_frame = detector.draw_detections(frame, boxes, confidences, class_ids)
        
        # Display FPS and detection count
        fps_counter += 1
        if fps_counter % 30 == 0:
            fps = fps_counter / (time.time() - start_time)
            print(f"FPS: {fps:.2f}")
        
        # Display info on image
        info_text = f"Detected {len(boxes)} persons"
        cv2.putText(annotated_frame, info_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # Display result
        cv2.imshow('Person Detection', annotated_frame)
        
        # Check exit condition
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # Clean up resources
    cap.release()
    cv2.destroyAllWindows()
    print("Program ended")

if __name__ == "__main__":
    main()
